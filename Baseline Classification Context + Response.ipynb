{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    
    ",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are 83 linguistic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>social</th>\n",
       "      <th>humans</th>\n",
       "      <th>funct</th>\n",
       "      <th>conj</th>\n",
       "      <th>cogmech</th>\n",
       "      <th>incl</th>\n",
       "      <th>verb</th>\n",
       "      <th>auxverb</th>\n",
       "      <th>future</th>\n",
       "      <th>discrep</th>\n",
       "      <th>past</th>\n",
       "      <th>inhib</th>\n",
       "      <th>preps</th>\n",
       "      <th>space</th>\n",
       "      <th>relativ</th>\n",
       "      <th>work</th>\n",
       "      <th>pronoun</th>\n",
       "      <th>ppron</th>\n",
       "      <th>you</th>\n",
       "      <th>affect</th>\n",
       "      <th>negemo</th>\n",
       "      <th>anx</th>\n",
       "      <th>adverb</th>\n",
       "      <th>anger</th>\n",
       "      <th>certain</th>\n",
       "      <th>present</th>\n",
       "      <th>cause</th>\n",
       "      <th>article</th>\n",
       "      <th>ipron</th>\n",
       "      <th>posemo</th>\n",
       "      <th>quant</th>\n",
       "      <th>tentat</th>\n",
       "      <th>excl</th>\n",
       "      <th>motion</th>\n",
       "      <th>insight</th>\n",
       "      <th>shehe</th>\n",
       "      <th>sad</th>\n",
       "      <th>achieve</th>\n",
       "      <th>they</th>\n",
       "      <th>negate</th>\n",
       "      <th>money</th>\n",
       "      <th>percept</th>\n",
       "      <th>see</th>\n",
       "      <th>number</th>\n",
       "      <th>hear</th>\n",
       "      <th>swear</th>\n",
       "      <th>time</th>\n",
       "      <th>filler</th>\n",
       "      <th>leisure</th>\n",
       "      <th>death</th>\n",
       "      <th>i</th>\n",
       "      <th>bio</th>\n",
       "      <th>sexual</th>\n",
       "      <th>health</th>\n",
       "      <th>friend</th>\n",
       "      <th>ingest</th>\n",
       "      <th>assent</th>\n",
       "      <th>we</th>\n",
       "      <th>body</th>\n",
       "      <th>feel</th>\n",
       "      <th>relig</th>\n",
       "      <th>home</th>\n",
       "      <th>family</th>\n",
       "      <th>nonfl</th>\n",
       "      <th>label</th>\n",
       "      <th>concat_tweet</th>\n",
       "      <th>context/0</th>\n",
       "      <th>context/1</th>\n",
       "      <th>response_emotion</th>\n",
       "      <th>context0_emotion</th>\n",
       "      <th>cos_sim</th>\n",
       "      <th>cos_sim1</th>\n",
       "      <th>Emotional_sim_response/context/0</th>\n",
       "      <th>response</th>\n",
       "      <th>noun_count_percent</th>\n",
       "      <th>adj_count_percent</th>\n",
       "      <th>adv_count_percent</th>\n",
       "      <th>pro_count_percent</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>upper_case_word_count</th>\n",
       "      <th>vader_pos</th>\n",
       "      <th>vader_neg</th>\n",
       "      <th>vader_neu</th>\n",
       "      <th>vader_compound</th>\n",
       "      <th>context1_emotion</th>\n",
       "      <th>val_diff_c0_resp</th>\n",
       "      <th>aro_diff_c0_resp</th>\n",
       "      <th>dom_diff_c0_resp</th>\n",
       "      <th>val_diff_c1_resp</th>\n",
       "      <th>aro_diff_c1_resp</th>\n",
       "      <th>dom_diff_c1_resp</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SARCASM</td>\n",
       "      <td>A minor child deserves privacy and should be k...</td>\n",
       "      <td>A minor child deserves privacy and should be k...</td>\n",
       "      <td>If your child isn't named Barron ... #BeBest...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.881</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.082695</td>\n",
       "      <td>I don't get this .. obviously you do car...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.112500</td>\n",
       "      <td>5.075000</td>\n",
       "      <td>0.063437</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.791</td>\n",
       "      <td>0.02995</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.893167</td>\n",
       "      <td>-0.533667</td>\n",
       "      <td>0.697667</td>\n",
       "      <td>-0.597500</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.331667</td>\n",
       "      <td>['A', 'minor', 'child', 'deserves', 'privacy',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SARCASM</td>\n",
       "      <td>Why is he a loser ? He's just a Press Secr...</td>\n",
       "      <td>Why is he a loser ? He's just a Press Secr...</td>\n",
       "      <td>having to make up excuses of why your crow...</td>\n",
       "      <td>anger</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.853</td>\n",
       "      <td>0.016047</td>\n",
       "      <td>trying to protest about . Talking about hi...</td>\n",
       "      <td>0.244444</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.155556</td>\n",
       "      <td>4.933333</td>\n",
       "      <td>0.109630</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.05840</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.028333</td>\n",
       "      <td>0.718333</td>\n",
       "      <td>-0.291667</td>\n",
       "      <td>-0.088333</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>['Why', 'is', 'he', 'a', 'loser', '?', 'He', \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SARCASM</td>\n",
       "      <td>Donald J . Trump is guilty as charged . The ev...</td>\n",
       "      <td>Donald J . Trump is guilty as charged . The ev...</td>\n",
       "      <td>I ’ ll remember to not support you at the bo...</td>\n",
       "      <td>anger</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.816</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.006215</td>\n",
       "      <td>He makes an insane about of money from t...</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.051724</td>\n",
       "      <td>0.017241</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>4.896552</td>\n",
       "      <td>0.084423</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.051724</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.732</td>\n",
       "      <td>0.16740</td>\n",
       "      <td>joy</td>\n",
       "      <td>-0.032500</td>\n",
       "      <td>1.403333</td>\n",
       "      <td>0.054167</td>\n",
       "      <td>-0.568333</td>\n",
       "      <td>2.245833</td>\n",
       "      <td>-0.135833</td>\n",
       "      <td>['Donald', 'J', '.', 'Trump', 'is', 'guilty', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SARCASM</td>\n",
       "      <td>Jamie Raskin tanked Doug Collins . Collins loo...</td>\n",
       "      <td>Jamie Raskin tanked Doug Collins . Collins loo...</td>\n",
       "      <td>But not half as stupid as Schiff looks . Peo...</td>\n",
       "      <td>anger</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.124028</td>\n",
       "      <td>Meanwhile Trump won't even release his SAT...</td>\n",
       "      <td>0.194805</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.116883</td>\n",
       "      <td>0.129870</td>\n",
       "      <td>5.610390</td>\n",
       "      <td>0.072862</td>\n",
       "      <td>0.155844</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.44215</td>\n",
       "      <td>joy</td>\n",
       "      <td>1.161111</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>0.964444</td>\n",
       "      <td>-0.630889</td>\n",
       "      <td>0.284222</td>\n",
       "      <td>-0.023556</td>\n",
       "      <td>['Jamie', 'Raskin', 'tanked', 'Doug', 'Collins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SARCASM</td>\n",
       "      <td>Man ... y ’ all gone “ both sides ” the apocal...</td>\n",
       "      <td>Man ... y ’ all gone “ both sides ” the apocal...</td>\n",
       "      <td>They already did . Obama said many times dur...</td>\n",
       "      <td>joy</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.080290</td>\n",
       "      <td>Pretty Sure the Anti-Lincoln Crowd Claimed...</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.053763</td>\n",
       "      <td>0.053763</td>\n",
       "      <td>0.043011</td>\n",
       "      <td>5.247312</td>\n",
       "      <td>0.056423</td>\n",
       "      <td>0.268817</td>\n",
       "      <td>0.021505</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.78595</td>\n",
       "      <td>anger</td>\n",
       "      <td>-0.466000</td>\n",
       "      <td>1.134000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>-0.740625</td>\n",
       "      <td>0.981875</td>\n",
       "      <td>-0.687500</td>\n",
       "      <td>['Man', '...', 'y', '’', 'all', 'gone', '“', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   social  humans  funct  conj  cogmech  incl  verb  auxverb  future  discrep  \\\n",
       "0    11.0     3.0   34.0   5.0     16.0   6.0  17.0     10.0     3.0      4.0   \n",
       "1     8.0     0.0   22.0   2.0      6.0   2.0   6.0      4.0     0.0      0.0   \n",
       "2     4.0     0.0   19.0   1.0      5.0   0.0   4.0      2.0     0.0      0.0   \n",
       "3    10.0     1.0   29.0   5.0     11.0   3.0  11.0      4.0     0.0      0.0   \n",
       "4     4.0     0.0   27.0   1.0      5.0   1.0   6.0      3.0     0.0      0.0   \n",
       "\n",
       "   past  inhib  preps  space  relativ  work  pronoun  ppron  you  affect  \\\n",
       "0   2.0    1.0    6.0    2.0      3.0   1.0      9.0    7.0  6.0     5.0   \n",
       "1   1.0    0.0    6.0    2.0      2.0   0.0      7.0    6.0  1.0     2.0   \n",
       "2   0.0    0.0    8.0    2.0      2.0   1.0      4.0    4.0  2.0     2.0   \n",
       "3   5.0    0.0    6.0    0.0      1.0   4.0     10.0    8.0  0.0     6.0   \n",
       "4   5.0    1.0    7.0    6.0     11.0   0.0      3.0    2.0  1.0     1.0   \n",
       "\n",
       "   negemo  anx  adverb  anger  certain  present  cause  article  ipron  \\\n",
       "0     2.0  1.0     2.0    1.0      2.0      9.0    1.0      1.0    2.0   \n",
       "1     2.0  0.0     3.0    1.0      0.0      4.0    3.0      2.0    1.0   \n",
       "2     1.0  1.0     1.0    0.0      1.0      4.0    1.0      4.0    0.0   \n",
       "3     3.0  0.0     2.0    3.0      1.0      6.0    3.0      1.0    2.0   \n",
       "4     0.0  0.0     2.0    0.0      2.0      1.0    0.0      5.0    1.0   \n",
       "\n",
       "   posemo  quant  tentat  excl  motion  insight  shehe  sad  achieve  they  \\\n",
       "0     3.0    1.0     1.0   1.0     1.0      1.0    1.0  0.0      0.0   0.0   \n",
       "1     0.0    0.0     0.0   1.0     0.0      0.0    3.0  1.0      2.0   2.0   \n",
       "2     1.0    0.0     0.0   1.0     0.0      2.0    2.0  0.0      0.0   0.0   \n",
       "3     3.0    0.0     0.0   3.0     0.0      1.0    5.0  0.0      2.0   3.0   \n",
       "4     1.0    5.0     0.0   0.0     1.0      1.0    0.0  0.0      1.0   1.0   \n",
       "\n",
       "   negate  money  percept  see  number  hear  swear  time  filler  leisure  \\\n",
       "0     0.0    0.0      0.0  0.0     0.0   0.0    0.0   0.0     0.0      0.0   \n",
       "1     0.0    0.0      0.0  0.0     0.0   0.0    0.0   0.0     0.0      0.0   \n",
       "2     1.0    1.0      0.0  0.0     0.0   0.0    0.0   0.0     0.0      0.0   \n",
       "3     2.0    0.0      5.0  4.0     1.0   1.0    1.0   1.0     0.0      0.0   \n",
       "4     0.0    0.0      1.0  0.0     2.0   1.0    0.0   5.0     1.0      0.0   \n",
       "\n",
       "   death    i  bio  sexual  health  friend  ingest  assent   we  body  feel  \\\n",
       "0    0.0  0.0  0.0     0.0     0.0     0.0     0.0     0.0  0.0   0.0   0.0   \n",
       "1    0.0  0.0  0.0     0.0     0.0     0.0     0.0     0.0  0.0   0.0   0.0   \n",
       "2    0.0  0.0  0.0     0.0     0.0     0.0     0.0     0.0  0.0   0.0   0.0   \n",
       "3    0.0  0.0  0.0     0.0     0.0     0.0     0.0     0.0  0.0   0.0   0.0   \n",
       "4    0.0  0.0  0.0     0.0     0.0     0.0     0.0     0.0  0.0   0.0   0.0   \n",
       "\n",
       "   relig  home  family  nonfl    label  \\\n",
       "0    0.0   0.0     0.0    0.0  SARCASM   \n",
       "1    0.0   0.0     0.0    0.0  SARCASM   \n",
       "2    0.0   0.0     0.0    0.0  SARCASM   \n",
       "3    0.0   0.0     0.0    0.0  SARCASM   \n",
       "4    0.0   0.0     0.0    0.0  SARCASM   \n",
       "\n",
       "                                        concat_tweet  \\\n",
       "0  A minor child deserves privacy and should be k...   \n",
       "1      Why is he a loser ? He's just a Press Secr...   \n",
       "2  Donald J . Trump is guilty as charged . The ev...   \n",
       "3  Jamie Raskin tanked Doug Collins . Collins loo...   \n",
       "4  Man ... y ’ all gone “ both sides ” the apocal...   \n",
       "\n",
       "                                           context/0  \\\n",
       "0  A minor child deserves privacy and should be k...   \n",
       "1      Why is he a loser ? He's just a Press Secr...   \n",
       "2  Donald J . Trump is guilty as charged . The ev...   \n",
       "3  Jamie Raskin tanked Doug Collins . Collins loo...   \n",
       "4  Man ... y ’ all gone “ both sides ” the apocal...   \n",
       "\n",
       "                                           context/1 response_emotion  \\\n",
       "0    If your child isn't named Barron ... #BeBest...          sadness   \n",
       "1      having to make up excuses of why your crow...            anger   \n",
       "2    I ’ ll remember to not support you at the bo...            anger   \n",
       "3    But not half as stupid as Schiff looks . Peo...            anger   \n",
       "4    They already did . Obama said many times dur...              joy   \n",
       "\n",
       "  context0_emotion  cos_sim  cos_sim1  Emotional_sim_response/context/0  \\\n",
       "0          sadness    0.881     0.880                          0.082695   \n",
       "1          sadness    0.775     0.853                          0.016047   \n",
       "2            anger    0.816     0.824                          0.006215   \n",
       "3          sadness    0.728     0.829                          0.124028   \n",
       "4          sadness    0.896     0.913                          0.080290   \n",
       "\n",
       "                                            response  noun_count_percent  \\\n",
       "0        I don't get this .. obviously you do car...            0.250000   \n",
       "1      trying to protest about . Talking about hi...            0.244444   \n",
       "2        He makes an insane about of money from t...            0.172414   \n",
       "3      Meanwhile Trump won't even release his SAT...            0.194805   \n",
       "4      Pretty Sure the Anti-Lincoln Crowd Claimed...            0.129032   \n",
       "\n",
       "   adj_count_percent  adv_count_percent  pro_count_percent  char_count  \\\n",
       "0           0.050000           0.125000           0.112500    5.075000   \n",
       "1           0.022222           0.066667           0.155556    4.933333   \n",
       "2           0.051724           0.017241           0.103448    4.896552   \n",
       "3           0.090909           0.116883           0.129870    5.610390   \n",
       "4           0.053763           0.053763           0.043011    5.247312   \n",
       "\n",
       "   word_density  punctuation_count  upper_case_word_count  vader_pos  \\\n",
       "0      0.063437           0.250000               0.025000      0.000   \n",
       "1      0.109630           0.111111               0.022222      0.000   \n",
       "2      0.084423           0.172414               0.051724      0.083   \n",
       "3      0.072862           0.155844               0.025974      0.125   \n",
       "4      0.056423           0.268817               0.021505      0.101   \n",
       "\n",
       "   vader_neg  vader_neu  vader_compound context1_emotion  val_diff_c0_resp  \\\n",
       "0      0.209      0.791         0.02995              joy          0.893167   \n",
       "1      0.227      0.773         0.05840          sadness          0.675000   \n",
       "2      0.185      0.732         0.16740              joy         -0.032500   \n",
       "3      0.131      0.744         0.44215              joy          1.161111   \n",
       "4      0.054      0.845         0.78595            anger         -0.466000   \n",
       "\n",
       "   aro_diff_c0_resp  dom_diff_c0_resp  val_diff_c1_resp  aro_diff_c1_resp  \\\n",
       "0         -0.533667          0.697667         -0.597500          0.015000   \n",
       "1          0.028333          0.718333         -0.291667         -0.088333   \n",
       "2          1.403333          0.054167         -0.568333          2.245833   \n",
       "3          0.288889          0.964444         -0.630889          0.284222   \n",
       "4          1.134000          0.010000         -0.740625          0.981875   \n",
       "\n",
       "   dom_diff_c1_resp                                     tokenized_text  \n",
       "0          0.331667  ['A', 'minor', 'child', 'deserves', 'privacy',...  \n",
       "1          0.416667  ['Why', 'is', 'he', 'a', 'loser', '?', 'He', \"...  \n",
       "2         -0.135833  ['Donald', 'J', '.', 'Trump', 'is', 'guilty', ...  \n",
       "3         -0.023556  ['Jamie', 'Raskin', 'tanked', 'Doug', 'Collins...  \n",
       "4         -0.687500  ['Man', '...', 'y', '’', 'all', 'gone', '“', '...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##TODO\n",
    "csv_file ='liwc_response_context_train.csv'   \n",
    "\n",
    "Train_df = pd.read_csv(csv_file)\n",
    "Train_df = Train_df.loc[:, ~Train_df.columns.str.contains('^Unnamed')]\n",
    "# print out the first few rows of data info\n",
    "Train_df.head(5)\n",
    "\n",
    "#These are the most useful features per SHAP \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotional_sim_response/context/0</th>\n",
       "      <th>Emotional_sim_response/context/1</th>\n",
       "      <th>funct</th>\n",
       "      <th>adverb</th>\n",
       "      <th>time</th>\n",
       "      <th>relativ</th>\n",
       "      <th>pronoun</th>\n",
       "      <th>ipron</th>\n",
       "      <th>affect</th>\n",
       "      <th>negemo</th>\n",
       "      <th>cogmech</th>\n",
       "      <th>discrep</th>\n",
       "      <th>verb</th>\n",
       "      <th>past</th>\n",
       "      <th>social</th>\n",
       "      <th>ppron</th>\n",
       "      <th>i</th>\n",
       "      <th>cause</th>\n",
       "      <th>they</th>\n",
       "      <th>auxverb</th>\n",
       "      <th>present</th>\n",
       "      <th>posemo</th>\n",
       "      <th>preps</th>\n",
       "      <th>excl</th>\n",
       "      <th>leisure</th>\n",
       "      <th>conj</th>\n",
       "      <th>incl</th>\n",
       "      <th>humans</th>\n",
       "      <th>certain</th>\n",
       "      <th>achieve</th>\n",
       "      <th>tentat</th>\n",
       "      <th>space</th>\n",
       "      <th>filler</th>\n",
       "      <th>shehe</th>\n",
       "      <th>anger</th>\n",
       "      <th>article</th>\n",
       "      <th>percept</th>\n",
       "      <th>hear</th>\n",
       "      <th>quant</th>\n",
       "      <th>we</th>\n",
       "      <th>motion</th>\n",
       "      <th>insight</th>\n",
       "      <th>feel</th>\n",
       "      <th>work</th>\n",
       "      <th>home</th>\n",
       "      <th>inhib</th>\n",
       "      <th>number</th>\n",
       "      <th>bio</th>\n",
       "      <th>body</th>\n",
       "      <th>swear</th>\n",
       "      <th>negate</th>\n",
       "      <th>you</th>\n",
       "      <th>future</th>\n",
       "      <th>assent</th>\n",
       "      <th>anx</th>\n",
       "      <th>nonfl</th>\n",
       "      <th>sad</th>\n",
       "      <th>money</th>\n",
       "      <th>see</th>\n",
       "      <th>relig</th>\n",
       "      <th>sexual</th>\n",
       "      <th>health</th>\n",
       "      <th>family</th>\n",
       "      <th>death</th>\n",
       "      <th>friend</th>\n",
       "      <th>ingest</th>\n",
       "      <th>label</th>\n",
       "      <th>context/0</th>\n",
       "      <th>context/1</th>\n",
       "      <th>concat_tweet</th>\n",
       "      <th>response_emotion</th>\n",
       "      <th>context0_emotion</th>\n",
       "      <th>cos_sim</th>\n",
       "      <th>cos_sim1</th>\n",
       "      <th>response</th>\n",
       "      <th>noun_count_percent</th>\n",
       "      <th>adj_count_percent</th>\n",
       "      <th>adv_count_percent</th>\n",
       "      <th>pro_count_percent</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>upper_case_word_count</th>\n",
       "      <th>vader_pos</th>\n",
       "      <th>vader_neg</th>\n",
       "      <th>vader_neu</th>\n",
       "      <th>vader_compound</th>\n",
       "      <th>context1_emotion</th>\n",
       "      <th>val_diff_c0_resp</th>\n",
       "      <th>aro_diff_c0_resp</th>\n",
       "      <th>dom_diff_c0_resp</th>\n",
       "      <th>val_diff_c1_resp</th>\n",
       "      <th>aro_diff_c1_resp</th>\n",
       "      <th>dom_diff_c1_resp</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.539945</td>\n",
       "      <td>0.033805</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NOT_SARCASM</td>\n",
       "      <td>Well now that ’ s problematic AF &lt;URL&gt;</td>\n",
       "      <td>My 5 year old ... asked me why they are ma...</td>\n",
       "      <td>Well now that ’ s problematic AF &lt;URL&gt;    My 5...</td>\n",
       "      <td>joy</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.828</td>\n",
       "      <td>My 3 year old , that just finished readi...</td>\n",
       "      <td>0.159091</td>\n",
       "      <td>0.113636</td>\n",
       "      <td>0.102273</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>4.647727</td>\n",
       "      <td>0.052815</td>\n",
       "      <td>0.261364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.698</td>\n",
       "      <td>0.82390</td>\n",
       "      <td>anger</td>\n",
       "      <td>3.091111</td>\n",
       "      <td>-0.618333</td>\n",
       "      <td>1.326667</td>\n",
       "      <td>-0.250556</td>\n",
       "      <td>0.063333</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>['Well', 'now', 'that', '’', 's', 'problematic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.789342</td>\n",
       "      <td>0.100227</td>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SARCASM</td>\n",
       "      <td>Last week the Fake News said that a section of...</td>\n",
       "      <td>The mainstream media doesn't report the fact...</td>\n",
       "      <td>Last week the Fake News said that a section of...</td>\n",
       "      <td>joy</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.849</td>\n",
       "      <td>How many verifiable lies has he told now ?...</td>\n",
       "      <td>0.169811</td>\n",
       "      <td>0.103774</td>\n",
       "      <td>0.075472</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>5.122642</td>\n",
       "      <td>0.048327</td>\n",
       "      <td>0.188679</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.798</td>\n",
       "      <td>0.46140</td>\n",
       "      <td>joy</td>\n",
       "      <td>-0.110952</td>\n",
       "      <td>-0.225714</td>\n",
       "      <td>-0.175238</td>\n",
       "      <td>-0.327143</td>\n",
       "      <td>0.161964</td>\n",
       "      <td>-0.268214</td>\n",
       "      <td>['Last', 'week', 'the', 'Fake', 'News', 'said'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.676308</td>\n",
       "      <td>0.198786</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SARCASM</td>\n",
       "      <td>Let ’ s Aplaud Brett When he deserves it he ...</td>\n",
       "      <td>He did try keep korkmaz in in the 4th quar...</td>\n",
       "      <td>Let ’ s Aplaud Brett When he deserves it he ...</td>\n",
       "      <td>anger</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.807</td>\n",
       "      <td>0.853</td>\n",
       "      <td>Maybe Docs just a scrub of a coach ... I...</td>\n",
       "      <td>0.168539</td>\n",
       "      <td>0.089888</td>\n",
       "      <td>0.056180</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>5.067416</td>\n",
       "      <td>0.056937</td>\n",
       "      <td>0.157303</td>\n",
       "      <td>0.044944</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.13345</td>\n",
       "      <td>anger</td>\n",
       "      <td>-1.513750</td>\n",
       "      <td>-0.527500</td>\n",
       "      <td>-0.505000</td>\n",
       "      <td>-0.158750</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>['Let', '’', 's', 'Aplaud', 'Brett', 'When', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.219656</td>\n",
       "      <td>0.038484</td>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NOT_SARCASM</td>\n",
       "      <td>Women generally hate this president . What's u...</td>\n",
       "      <td>I've hated him before he was placed in offic...</td>\n",
       "      <td>Women generally hate this president . What's u...</td>\n",
       "      <td>anger</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.814</td>\n",
       "      <td>0.892</td>\n",
       "      <td>is just a cover up for the real hate insid...</td>\n",
       "      <td>0.160494</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.061728</td>\n",
       "      <td>0.123457</td>\n",
       "      <td>4.530864</td>\n",
       "      <td>0.055937</td>\n",
       "      <td>0.172840</td>\n",
       "      <td>0.024691</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.12500</td>\n",
       "      <td>anger</td>\n",
       "      <td>1.579000</td>\n",
       "      <td>-1.195000</td>\n",
       "      <td>1.049000</td>\n",
       "      <td>-0.767333</td>\n",
       "      <td>0.451333</td>\n",
       "      <td>-0.548000</td>\n",
       "      <td>['Women', 'generally', 'hate', 'this', 'presid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.812410</td>\n",
       "      <td>0.203423</td>\n",
       "      <td>48</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NOT_SARCASM</td>\n",
       "      <td>Dear media Remoaners , you excitedly sharing c...</td>\n",
       "      <td>When Spiked claim that Brexiteers knew exact...</td>\n",
       "      <td>Dear media Remoaners , you excitedly sharing c...</td>\n",
       "      <td>anger</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0.875</td>\n",
       "      <td>The irony being that he even has to ask ...</td>\n",
       "      <td>0.165138</td>\n",
       "      <td>0.082569</td>\n",
       "      <td>0.100917</td>\n",
       "      <td>0.100917</td>\n",
       "      <td>5.018349</td>\n",
       "      <td>0.046040</td>\n",
       "      <td>0.165138</td>\n",
       "      <td>0.009174</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.82370</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.328077</td>\n",
       "      <td>-0.374615</td>\n",
       "      <td>-0.044038</td>\n",
       "      <td>0.297667</td>\n",
       "      <td>-0.080000</td>\n",
       "      <td>-0.069167</td>\n",
       "      <td>['Dear', 'media', 'Remoaners', ',', 'you', 'ex...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Emotional_sim_response/context/0  Emotional_sim_response/context/1  funct  \\\n",
       "0                          0.539945                          0.033805     25   \n",
       "1                          0.789342                          0.100227     33   \n",
       "2                          0.676308                          0.198786     29   \n",
       "3                          0.219656                          0.038484     41   \n",
       "4                          0.812410                          0.203423     48   \n",
       "\n",
       "   adverb  time  relativ  pronoun  ipron  affect  negemo  cogmech  discrep  \\\n",
       "0       4     9        9       12      6       6       2       12        1   \n",
       "1       4     6       12        5      3       5       2       10        0   \n",
       "2       3     4        9        5      2       6       4        9        1   \n",
       "3       3     2       11       13      6       7       4       12        0   \n",
       "4       6     0        6       12      5       9       5       17        0   \n",
       "\n",
       "   verb  past  social  ppron  i  cause  they  auxverb  present  posemo  preps  \\\n",
       "0     4     2       9      6  2      5     3        1        2       4      4   \n",
       "1     8     4       4      2  0      3     0        5        4       3      9   \n",
       "2    10     6       5      3  0      0     0        4        4       2      7   \n",
       "3    10     2      11      7  0      0     0        5        6       3     13   \n",
       "4    10     3       9      7  0      1     4        8        3       4     13   \n",
       "\n",
       "   excl  leisure  conj  incl  humans  certain  achieve  tentat  space  filler  \\\n",
       "0     2        2     5     2       1        1        4       1      1       1   \n",
       "1     2        0     1     0       0        5        2       0      5       0   \n",
       "2     2        5     2     1       0        0        2       0      4       0   \n",
       "3     1        0     4     6       3        2        2       1      9       0   \n",
       "4     4        0     4     1       0        5        0       5      6       0   \n",
       "\n",
       "   shehe  anger  article  percept  hear  quant  we  motion  insight  feel  \\\n",
       "0      1      1        0        0     0      0   0       0        0     0   \n",
       "1      1      1        8        3     2      2   1       1        1     1   \n",
       "2      3      3        7        0     0      0   0       1        1     0   \n",
       "3      4      4        3        0     0      2   2       0        1     0   \n",
       "4      1      3        3        1     0      4   0       0        3     1   \n",
       "\n",
       "   work  home  inhib  number  bio  body  swear  negate  you  future  assent  \\\n",
       "0     0     0      0       0    0     0      0       0    0       0       0   \n",
       "1     1     1      0       0    0     0      0       0    0       0       0   \n",
       "2     1     0      3       1    1     1      1       1    0       0       0   \n",
       "3     2     0      1       0    0     0      0       0    1       2       1   \n",
       "4     0     0      1       0    0     0      0       2    2       1       0   \n",
       "\n",
       "   anx  nonfl  sad  money  see  relig  sexual  health  family  death  friend  \\\n",
       "0    0      0    0      0    0      0       0       0       0      0       0   \n",
       "1    0      0    0      0    0      0       0       0       0      0       0   \n",
       "2    0      0    0      0    0      0       0       0       0      0       0   \n",
       "3    0      0    0      0    0      0       0       0       0      0       0   \n",
       "4    1      1    0      0    0      0       0       0       0      0       0   \n",
       "\n",
       "   ingest        label                                          context/0  \\\n",
       "0       0  NOT_SARCASM             Well now that ’ s problematic AF <URL>   \n",
       "1       0      SARCASM  Last week the Fake News said that a section of...   \n",
       "2       0      SARCASM    Let ’ s Aplaud Brett When he deserves it he ...   \n",
       "3       0  NOT_SARCASM  Women generally hate this president . What's u...   \n",
       "4       0  NOT_SARCASM  Dear media Remoaners , you excitedly sharing c...   \n",
       "\n",
       "                                           context/1  \\\n",
       "0      My 5 year old ... asked me why they are ma...   \n",
       "1    The mainstream media doesn't report the fact...   \n",
       "2      He did try keep korkmaz in in the 4th quar...   \n",
       "3    I've hated him before he was placed in offic...   \n",
       "4    When Spiked claim that Brexiteers knew exact...   \n",
       "\n",
       "                                        concat_tweet response_emotion  \\\n",
       "0  Well now that ’ s problematic AF <URL>    My 5...              joy   \n",
       "1  Last week the Fake News said that a section of...              joy   \n",
       "2    Let ’ s Aplaud Brett When he deserves it he ...            anger   \n",
       "3  Women generally hate this president . What's u...            anger   \n",
       "4  Dear media Remoaners , you excitedly sharing c...            anger   \n",
       "\n",
       "  context0_emotion  cos_sim  cos_sim1  \\\n",
       "0            anger    0.817     0.828   \n",
       "1             fear    0.859     0.849   \n",
       "2              joy    0.807     0.853   \n",
       "3            anger    0.814     0.892   \n",
       "4            anger    0.861     0.875   \n",
       "\n",
       "                                            response  noun_count_percent  \\\n",
       "0        My 3 year old , that just finished readi...            0.159091   \n",
       "1      How many verifiable lies has he told now ?...            0.169811   \n",
       "2        Maybe Docs just a scrub of a coach ... I...            0.168539   \n",
       "3      is just a cover up for the real hate insid...            0.160494   \n",
       "4        The irony being that he even has to ask ...            0.165138   \n",
       "\n",
       "   adj_count_percent  adv_count_percent  pro_count_percent  char_count  \\\n",
       "0           0.113636           0.102273           0.090909    4.647727   \n",
       "1           0.103774           0.075472           0.037736    5.122642   \n",
       "2           0.089888           0.056180           0.067416    5.067416   \n",
       "3           0.037037           0.061728           0.123457    4.530864   \n",
       "4           0.082569           0.100917           0.100917    5.018349   \n",
       "\n",
       "   word_density  punctuation_count  upper_case_word_count  vader_pos  \\\n",
       "0      0.052815           0.261364               0.045455      0.171   \n",
       "1      0.048327           0.188679               0.018868      0.105   \n",
       "2      0.056937           0.157303               0.044944      0.058   \n",
       "3      0.055937           0.172840               0.024691      0.131   \n",
       "4      0.046040           0.165138               0.009174      0.133   \n",
       "\n",
       "   vader_neg  vader_neu  vader_compound context1_emotion  val_diff_c0_resp  \\\n",
       "0      0.131      0.698         0.82390            anger          3.091111   \n",
       "1      0.097      0.798         0.46140              joy         -0.110952   \n",
       "2      0.130      0.812         0.13345            anger         -1.513750   \n",
       "3      0.182      0.688         0.12500            anger          1.579000   \n",
       "4      0.091      0.775         0.82370            anger          0.328077   \n",
       "\n",
       "   aro_diff_c0_resp  dom_diff_c0_resp  val_diff_c1_resp  aro_diff_c1_resp  \\\n",
       "0         -0.618333          1.326667         -0.250556          0.063333   \n",
       "1         -0.225714         -0.175238         -0.327143          0.161964   \n",
       "2         -0.527500         -0.505000         -0.158750          0.330000   \n",
       "3         -1.195000          1.049000         -0.767333          0.451333   \n",
       "4         -0.374615         -0.044038          0.297667         -0.080000   \n",
       "\n",
       "   dom_diff_c1_resp                                     tokenized_text  \n",
       "0          0.130000  ['Well', 'now', 'that', '’', 's', 'problematic...  \n",
       "1         -0.268214  ['Last', 'week', 'the', 'Fake', 'News', 'said'...  \n",
       "2          0.012500  ['Let', '’', 's', 'Aplaud', 'Brett', 'When', '...  \n",
       "3         -0.548000  ['Women', 'generally', 'hate', 'this', 'presid...  \n",
       "4         -0.069167  ['Dear', 'media', 'Remoaners', ',', 'you', 'ex...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##TODO\n",
    "\n",
    "csv_file = 'liwc_response_context_test.csv'\n",
    "\n",
    "Test_df = pd.read_csv(csv_file)\n",
    "Test_df = Test_df.loc[:, ~Test_df.columns.str.contains('^Unnamed')]\n",
    "# print out the first few rows of data info\n",
    "Test_df.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 95)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 94)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = Train_df\n",
    "\n",
    "X_test= Test_df\n",
    "\n",
    "y_train= Train_df['label']\n",
    "\n",
    "y_test = Test_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import preprocessing\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}') # word that appears 1 or more times\n",
    "count_vect.fit(Train_df['concat_tweet'])\n",
    "count_vect.fit(Test_df['concat_tweet'])\n",
    "# to show resulting vocabulary; the numbers are not counts, they are the position in the sparse vector.\n",
    "count_vect.vocabulary_\n",
    "\n",
    "# transform the training and validation data using count vectorizer object: doc x term\n",
    "xtrain_count =  count_vect.transform(Train_df['concat_tweet']) \n",
    "xvalid_count =  count_vect.transform(Test_df['concat_tweet'])\n",
    "\n",
    "xvalid_count= xvalid_count.toarray()\n",
    "xtrain_count= xtrain_count.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word level tfidf Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000) # considers the top 5000 most frequent features\n",
    "tfidf_vect.fit(Train_df['concat_tweet'])\n",
    "tfidf_vect.fit(Test_df['concat_tweet'])\n",
    "\n",
    "\n",
    "tfidf_train = tfidf_vect.transform(Train_df['concat_tweet'])\n",
    "tfidf_test = tfidf_vect.transform(Test_df['concat_tweet'])\n",
    "\n",
    "tfidf_train = tfidf_train.toarray()\n",
    "tfidf_test = tfidf_test.toarray()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ngram level tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(Train_df['concat_tweet'])\n",
    "tfidf_vect_ngram.fit(Test_df['concat_tweet'])\n",
    "\n",
    "tfidf_ngram_train =  tfidf_vect_ngram.transform(Train_df['concat_tweet'])\n",
    "tfidf_ngram_test =  tfidf_vect_ngram.transform(Test_df['concat_tweet'])\n",
    "\n",
    "tfidf_ngram_train = tfidf_ngram_train.toarray()\n",
    "tfidf_ngram_test = tfidf_ngram_test.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character level tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Martina/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:501: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/Users/Martina/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:501: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "  ### characters level tf-idf\n",
    "\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(Train_df['concat_tweet'])\n",
    "tfidf_vect_ngram_chars.fit(Test_df['concat_tweet'])\n",
    "\n",
    "\n",
    "tfidf_ngram_chars_train =  tfidf_vect_ngram_chars.transform(Train_df['concat_tweet']) \n",
    "\n",
    "tfidf_ngram_chars_test =  tfidf_vect_ngram_chars.transform(Test_df['concat_tweet'])\n",
    "                                   \n",
    "\n",
    "                                   \n",
    "tfidf_ngram_chars_train =  tfidf_ngram_chars_train.toarray()\n",
    "\n",
    "tfidf_ngram_chars_test = tfidf_ngram_chars_test.toarray()\n",
    "                                           \n",
    "                                   \n",
    "                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "# fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "\n",
    "    return metrics.accuracy_score(predictions, y_test),metrics.precision_score(predictions, y_test),metrics.recall_score(predictions, y_test),metrics.f1_score(predictions, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results on Test data running only count vectors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  word level tfidf feature NB, :  A: 0.57 P: 0.6 R: 0.56 F1: 0.58\n",
      " Count vector feature NB, :  A: 0.59 P: 0.57 R: 0.59 F1: 0.58\n",
      " ngram word level tfidf feature NB, :  A: 0.57 P: 0.6 R: 0.56 F1: 0.58\n",
      " ngram char level tfidf feature NB, :  A: 0.61 P: 0.72 R: 0.59 F1: 0.65\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "accuracy, precision, recall,f1  = train_model(naive_bayes.GaussianNB(), tfidf_ngram_train , y_train, tfidf_ngram_test)\n",
    "print (\"  word level tfidf feature NB, : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n",
    "\n",
    "accuracy, precision, recall,f1  = train_model(naive_bayes.GaussianNB(), xtrain_count , y_train, xvalid_count)\n",
    "print (\" Count vector feature NB, : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n",
    "\n",
    "accuracy, precision, recall,f1  = train_model(naive_bayes.GaussianNB(), tfidf_ngram_train , y_train, tfidf_ngram_test)\n",
    "print (\" ngram word level tfidf feature NB, : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n",
    "\n",
    "accuracy, precision, recall,f1  = train_model(naive_bayes.GaussianNB(), tfidf_ngram_chars_train , y_train, tfidf_ngram_chars_test)\n",
    "print (\" ngram char level tfidf feature NB, : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word level tfidf feature RF :  A: 0.6 P: 0.73 R: 0.58 F1: 0.65\n",
      "Count vector feature RF, :  A: 0.62 P: 0.67 R: 0.61 F1: 0.64\n",
      "ngram word level tfidf feature RF, :  A: 0.6 P: 0.73 R: 0.58 F1: 0.65\n",
      "ngram char level tfidf feature RF, :  A: 0.65 P: 0.62 R: 0.66 F1: 0.64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "accuracy, precision, recall,f1  = train_model(RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0), tfidf_ngram_train , y_train, tfidf_ngram_test)\n",
    "print (\"word level tfidf feature RF : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n",
    "\n",
    "accuracy, precision, recall,f1  = train_model(RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0), xtrain_count , y_train, xvalid_count)\n",
    "print (\"Count vector feature RF, : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n",
    "\n",
    "accuracy, precision, recall,f1  = train_model(RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0), tfidf_ngram_train , y_train, tfidf_ngram_test)\n",
    "print (\"ngram word level tfidf feature RF, : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n",
    "\n",
    "accuracy, precision, recall,f1  = train_model(RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0), tfidf_ngram_chars_train , y_train, tfidf_ngram_chars_test)\n",
    "print (\"ngram char level tfidf feature RF, : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word level tfidf feature SVM :  A: 0.62 P: 0.77 R: 0.59 F1: 0.67\n",
      "Count vector feature SVM, :  A: 0.63 P: 0.61 R: 0.64 F1: 0.63\n",
      "ngram word level tfidf feature SVM, :  A: 0.62 P: 0.77 R: 0.59 F1: 0.67\n",
      "ngram char level tfidf feature SVM, :  A: 0.67 P: 0.7 R: 0.66 F1: 0.68\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "accuracy, precision, recall,f1  = train_model(svm.SVC(), tfidf_ngram_train , y_train, tfidf_ngram_test)\n",
    "print (\"word level tfidf feature SVM : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n",
    "\n",
    "accuracy, precision, recall,f1  = train_model(svm.SVC(), xtrain_count , y_train, xvalid_count)\n",
    "print (\"Count vector feature SVM, : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n",
    "\n",
    "accuracy, precision, recall,f1  = train_model(svm.SVC(), tfidf_ngram_train , y_train, tfidf_ngram_test)\n",
    "print (\"ngram word level tfidf feature SVM, : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n",
    "\n",
    "accuracy, precision, recall,f1  = train_model(svm.SVC(), tfidf_ngram_chars_train , y_train, tfidf_ngram_chars_test)\n",
    "print (\"ngram char level tfidf feature SVM, : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_count_features_train = np.column_stack([xtrain_count,tfidf_ngram_train,tfidf_ngram_train,tfidf_ngram_chars_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_count_features_test = np.column_stack([tfidf_ngram_test,xvalid_count,tfidf_ngram_test,tfidf_ngram_chars_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing only count vector combination "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all count features combo RF :  A: 0.64 P: 0.65 R: 0.64 F1: 0.64\n",
      " all count features combo NB, :  A: 0.49 P: 0.21 R: 0.47 F1: 0.3\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall,f1  = train_model(RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0), all_count_features_train, y_train, all_count_features_test)\n",
    "print (\"all count features combo RF : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n",
    "\n",
    "accuracy, precision, recall,f1  = train_model(naive_bayes.GaussianNB(), all_count_features_train , y_train, all_count_features_test)\n",
    "print (\" all count features combo NB, : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all count features combo SVM :  A: 0.52 P: 0.51 R: 0.52 F1: 0.52\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall,f1  = train_model(svm.SVC(), all_count_features_train , y_train, all_count_features_test)\n",
    "print (\"all count features combo SVM : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Linguistic Features to Count Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.drop(['tokenized_text','label','concat_tweet','context/0','context/1','response'], axis=1)\n",
    "X_train = X_train.drop(['tokenized_text','label','concat_tweet','context/0','context/1','response'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['response_emotion'] = X_test['response_emotion'].astype('category').cat.codes\n",
    "X_test['context1_emotion'] = X_test['context1_emotion'].astype('category').cat.codes\n",
    "X_test['context0_emotion'] = X_test['context0_emotion'].astype('category').cat.codes\n",
    "\n",
    "\n",
    "\n",
    "X_train['context1_emotion'] = X_train['context1_emotion'].astype('category').cat.codes\n",
    "X_train['response_emotion'] = X_train['response_emotion'].astype('category').cat.codes\n",
    "X_train['context0_emotion'] = X_train['context0_emotion'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALL Linguistic Features + ALL count features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_train = np.column_stack([all_count_features_train, X_train])\n",
    "all_features_test = np.column_stack([all_count_features_test, X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linguistic + count features RF :  A: 0.66 P: 0.8 R: 0.62 F1: 0.7\n",
      "Linguistic + count features NB, :  A: 0.49 P: 0.22 R: 0.48 F1: 0.3\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall,f1  = train_model(RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0), all_features_train, y_train, all_features_test)\n",
    "print (\"Linguistic + count features RF : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n",
    "\n",
    "accuracy, precision, recall,f1  = train_model(naive_bayes.GaussianNB(), all_features_train , y_train, all_features_test)\n",
    "print (\"Linguistic + count features NB, : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linguistic + count features SVM :  A: 0.52 P: 0.68 R: 0.52 F1: 0.59\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall,f1  = train_model(svm.SVC(), all_features_train , y_train, all_features_test)\n",
    "print (\"Linguistic + count features SVM : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Linguistic Features + Ngram Character tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ling_ngram_chars_train = np.column_stack([tfidf_ngram_chars_train, X_train])\n",
    "all_ling_ngram_chars_test = np.column_stack([tfidf_ngram_chars_test, X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all Linguistic Features + Ngram Char level tfidfRF :  A: 0.66 P: 0.77 R: 0.64 F1: 0.69\n",
      "all Linguistic Features + Ngram Char level tfidf NB, :  A: 0.61 P: 0.66 R: 0.6 F1: 0.63\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall,f1  = train_model(RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0), all_ling_ngram_chars_train, y_train, all_ling_ngram_chars_test)\n",
    "print (\"all Linguistic Features + Ngram Char level tfidfRF : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n",
    "\n",
    "accuracy, precision, recall,f1  = train_model(naive_bayes.GaussianNB(), all_ling_ngram_chars_train , y_train, all_ling_ngram_chars_test)\n",
    "print (\"all Linguistic Features + Ngram Char level tfidf NB, : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all Linguistic Features + Ngram Char level tfidf SVM :  A: 0.53 P: 0.54 R: 0.53 F1: 0.54\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall,f1  = train_model(svm.SVC(), all_ling_ngram_chars_train , y_train, all_ling_ngram_chars_test)\n",
    "print (\"all Linguistic Features + Ngram Char level tfidf SVM : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Linguistic Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_df_select = Train_df[['vader_compound','vader_pos','Emotional_sim_response/context/0','you','noun_count_percent','upper_case_word_count','char_count','adv_count_percent','time','context1_emotion','incl','cos_sim','pro_count_percent','ipron','vader_neg','context0_emotion','dom_diff_c0_resp','Emotional_sim_response/context/1','response_emotion','aro_diff_c1_resp']].copy()\n",
    "\n",
    "Test_df_select = Test_df[['vader_compound','vader_pos','Emotional_sim_response/context/0','you','noun_count_percent','upper_case_word_count','char_count','adv_count_percent','time','context1_emotion','incl','cos_sim','pro_count_percent','ipron','vader_neg','context0_emotion','dom_diff_c0_resp','Emotional_sim_response/context/1','response_emotion','aro_diff_c1_resp']].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_df_select ['context1_emotion'] = Train_df_select ['context1_emotion'].astype('category').cat.codes\n",
    "Test_df_select['context1_emotion'] = Test_df_select['context1_emotion'].astype('category').cat.codes\n",
    "\n",
    "Train_df_select ['context0_emotion'] = Train_df_select ['context0_emotion'].astype('category').cat.codes\n",
    "Test_df_select['context0_emotion'] = Test_df_select['context0_emotion'].astype('category').cat.codes\n",
    "\n",
    "Train_df_select ['response_emotion'] = Train_df_select ['response_emotion'].astype('category').cat.codes\n",
    "Test_df_select['response_emotion'] = Test_df_select['response_emotion'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select Linguistic Features RF :  A: 0.62 P: 0.68 R: 0.6 F1: 0.64\n",
      "Select Linguistic Features NB, :  A: 0.63 P: 0.64 R: 0.63 F1: 0.64\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall,f1  = train_model(RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0), Train_df_select, y_train, Test_df_select)\n",
    "print (\"Select Linguistic Features RF : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n",
    "\n",
    "accuracy, precision, recall,f1  = train_model(naive_bayes.GaussianNB(), Train_df_select, y_train, Test_df_select)\n",
    "print (\"Select Linguistic Features NB, : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select Linguistic Features SVM :  A: 0.58 P: 0.42 R: 0.62 F1: 0.5\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall,f1  = train_model(svm.SVC(), Train_df_select, y_train, Test_df_select)\n",
    "print (\"Select Linguistic Features SVM : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Linguistic Features + All counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_ling_all_count_train = np.column_stack([all_count_features_train, Train_df_select])\n",
    "select_ling_all_count_test = np.column_stack([all_count_features_test, Test_df_select])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select Linguistic + ALL counts RF :  A: 0.66 P: 0.7 R: 0.65 F1: 0.67\n",
      "Select Linguistic + ALL counts NB, :  A: 0.49 P: 0.21 R: 0.47 F1: 0.3\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall,f1  = train_model(RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0), select_ling_all_count_train, y_train, select_ling_all_count_test)\n",
    "print (\"Select Linguistic + ALL counts RF : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n",
    "\n",
    "accuracy, precision, recall,f1  = train_model(naive_bayes.GaussianNB(), select_ling_all_count_train, y_train, select_ling_all_count_test)\n",
    "print (\"Select Linguistic + ALL counts NB, : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select Linguistic + ALL counts SVM :  A: 0.57 P: 0.71 R: 0.56 F1: 0.62\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall,f1  = train_model(svm.SVC(), select_ling_all_count_train, y_train, select_ling_all_count_test)\n",
    "print (\"Select Linguistic + ALL counts SVM : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Linguistic Features + ngram char tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_ling_ngram_char_train = np.column_stack([tfidf_ngram_chars_train, Train_df_select])\n",
    "select_ling_ngram_char_test = np.column_stack([tfidf_ngram_chars_test, Test_df_select])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select Linguistic + tfidf ngram char RF :  A: 0.64 P: 0.63 R: 0.65 F1: 0.64\n",
      "Select Linguistic + tfidf ngram char NB, :  A: 0.6 P: 0.68 R: 0.59 F1: 0.63\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall,f1  = train_model(RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0), select_ling_ngram_char_train, y_train, select_ling_ngram_char_test)\n",
    "print (\"Select Linguistic + tfidf ngram char RF : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n",
    "\n",
    "accuracy, precision, recall,f1  = train_model(naive_bayes.GaussianNB(), select_ling_ngram_char_train, y_train, select_ling_ngram_char_test)\n",
    "print (\"Select Linguistic + tfidf ngram char NB, : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select Linguistic + tfidf ngram char SVM, :  A: 0.63 P: 0.58 R: 0.64 F1: 0.61\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall,f1  = train_model(svm.SVC(), select_ling_ngram_char_train, y_train, select_ling_ngram_char_test)\n",
    "print (\"Select Linguistic + tfidf ngram char SVM, : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate Select Features Response & Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_df_select_uni = Train_df[['word_density','vader_neg','vader_compound','posemo','vader_pos','ppron','funct','relativ','incl','time','response_emotion','i','preps','pronoun','cos_sim','char_count','context1_emotion','affect','bio','cos_sim1']].copy()\n",
    "\n",
    "Test_df_select_uni = Test_df[['word_density','vader_neg','vader_compound','posemo','vader_pos','ppron','funct','relativ','incl','time','response_emotion','i','preps','pronoun','cos_sim','char_count','context1_emotion','affect','bio','cos_sim1']].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_df_select_uni['context1_emotion'] = Train_df_select_uni['context1_emotion'].astype('category').cat.codes\n",
    "Test_df_select_uni['context1_emotion'] = Test_df_select_uni['context1_emotion'].astype('category').cat.codes\n",
    "\n",
    "\n",
    "Train_df_select_uni['response_emotion'] = Train_df_select_uni['response_emotion'].astype('category').cat.codes\n",
    "Test_df_select_uni['response_emotion'] = Test_df_select_uni['response_emotion'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select Linguistic Features RF :  A: 0.51 P: 0.42 R: 0.51 F1: 0.46\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall,f1  = train_model(RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0), Train_df_select_uni, y_train, Test_df_select_uni)\n",
    "print (\"Select Linguistic Features RF : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate Select Features Response & Context ngram char tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_ling_ngram_char_train_uni = np.column_stack([tfidf_ngram_chars_train, Train_df_select_uni])\n",
    "select_ling_ngram_char_test_uni = np.column_stack([tfidf_ngram_chars_test, Test_df_select_uni])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select Linguistic + tfidf ngram char RF :  A: 0.65 P: 0.61 R: 0.67 F1: 0.64\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall,f1  = train_model(RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0), select_ling_ngram_char_train_uni, y_train, select_ling_ngram_char_test_uni)\n",
    "print (\"Select Linguistic + tfidf ngram char RF : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate Select Features Response & Context + All Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_ling_all_count_train_uni = np.column_stack([all_count_features_train, Train_df_select_uni])\n",
    "select_ling_all_count_test_uni = np.column_stack([all_count_features_test, Test_df_select_uni])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select Linguistic + ALL counts RF :  A: 0.66 P: 0.64 R: 0.66 F1: 0.65\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall,f1  = train_model(RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0), select_ling_all_count_train_uni, y_train, select_ling_all_count_test_uni)\n",
    "print (\"Select Linguistic + ALL counts RF : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance (RF) Select Features Response & Context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_df_select_fi = Train_df[['word_density','vader_compound','response_emotion','char_count','vader_neg','vader_pos','cos_sim','punctuation_count','upper_case_word_count','adv_count_percent','context1_emotion','vader_neu','relativ','pro_count_percent','noun_count_percent','val_diff_c1_resp','aro_diff_c0_resp','dom_diff_c1_resp','dom_diff_c0_resp','Emotional_sim_response/context/0']].copy()\n",
    "\n",
    "Test_df_select_fi = Test_df[['word_density','vader_compound','response_emotion','char_count','vader_neg','vader_pos','cos_sim','punctuation_count','upper_case_word_count','adv_count_percent','context1_emotion','vader_neu','relativ','pro_count_percent','noun_count_percent','val_diff_c1_resp','aro_diff_c0_resp','dom_diff_c1_resp','dom_diff_c0_resp','Emotional_sim_response/context/0']].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_df_select_fi['context1_emotion'] = Train_df_select_fi['context1_emotion'].astype('category').cat.codes\n",
    "Test_df_select_fi['context1_emotion'] = Test_df_select_fi['context1_emotion'].astype('category').cat.codes\n",
    "\n",
    "\n",
    "Train_df_select_fi['response_emotion'] = Train_df_select_fi['response_emotion'].astype('category').cat.codes\n",
    "Test_df_select_fi['response_emotion'] = Test_df_select_fi['response_emotion'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select Linguistic Features RF :  A: 0.61 P: 0.63 R: 0.61 F1: 0.62\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall,f1  = train_model(RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0), Train_df_select_fi, y_train, Test_df_select_fi)\n",
    "print (\"Select Linguistic Features RF : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feaure Importance RF Select Features Response & Context ngram char tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_ling_ngram_char_train_fi = np.column_stack([tfidf_ngram_chars_train, Train_df_select_fi])\n",
    "select_ling_ngram_char_test_fi = np.column_stack([tfidf_ngram_chars_test, Test_df_select_fi])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select Linguistic + tfidf ngram char RF :  A: 0.66 P: 0.63 R: 0.66 F1: 0.65\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall,f1  = train_model(RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0), select_ling_ngram_char_train_fi, y_train, select_ling_ngram_char_test_fi)\n",
    "print (\"Select Linguistic + tfidf ngram char RF : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance RF Select Features Response & Context + All Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_ling_all_count_train_fi = np.column_stack([all_count_features_train, Train_df_select_fi])\n",
    "select_ling_all_count_test_fi = np.column_stack([all_count_features_test, Test_df_select_fi])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select Linguistic + ALL counts RF :  A: 0.65 P: 0.65 R: 0.65 F1: 0.65\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall,f1  = train_model(RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0), select_ling_all_count_train_fi, y_train, select_ling_all_count_test_fi)\n",
    "print (\"Select Linguistic + ALL counts RF : \", \"A:\", round(accuracy,2), \"P:\", round(precision,2), \"R:\", round(recall,2), \"F1:\", round(f1,2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
